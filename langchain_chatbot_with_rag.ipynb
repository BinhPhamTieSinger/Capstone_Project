{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# Building a Chatbot That Knows Your Documents (RAG)\n",
        "\n",
        "In the previous notebook, you built a chatbot that could chat and use tools. But what if you want your chatbot to answer questions about YOUR specific documents?\n",
        "\n",
        "This is where **RAG** (Retrieval-Augmented Generation) comes in!\n",
        "\n",
        "## What You'll Learn in This Notebook\n",
        "\n",
        "1. **What is RAG?** - The technique that powers ChatGPT's document analysis\n",
        "2. **Embeddings** - How to turn text into numbers that capture meaning\n",
        "3. **Vector Stores** - Databases designed for similarity search\n",
        "4. **Basic RAG Chain** - Combining search with generation using LCEL\n",
        "5. **History-Aware RAG** - Handling follow-up questions correctly\n",
        "6. **Advanced RAG with LangGraph** - Building sophisticated RAG pipelines\n",
        "\n",
        "## The Problem RAG Solves\n",
        "\n",
        "```\n",
        "WITHOUT RAG:\n",
        "------------------------------------------------------------\n",
        "User: \"What's in the LLaMA-2 research paper?\"\n",
        "AI: \"I don't have access to specific papers...\n",
        "     Generally, LLaMA-2 is a language model...\"\n",
        "\n",
        "     [X] Generic answer - NOT helpful!\n",
        "------------------------------------------------------------\n",
        "\n",
        "WITH RAG:\n",
        "------------------------------------------------------------\n",
        "User: \"What's in the LLaMA-2 research paper?\"\n",
        "AI: [Searches your documents -> Finds relevant chunks]\n",
        "    \"According to the LLaMA-2 paper, the model was trained\n",
        "     on 2 trillion tokens with a context length of 4096...\"\n",
        "\n",
        "     [OK] Specific answer from YOUR documents!\n",
        "------------------------------------------------------------\n",
        "```\n",
        "\n",
        "## The RAG Architecture\n",
        "\n",
        "```\n",
        "+---------------------------------------------------------------------+\n",
        "|                         RAG PIPELINE                                 |\n",
        "+---------------------------------------------------------------------+\n",
        "|                                                                      |\n",
        "|   OFFLINE (One-time setup):                                          |\n",
        "|   +----------+    +----------+    +----------+    +----------+       |\n",
        "|   |Documents | -> |  Split   | -> |  Embed   | -> | Store in |       |\n",
        "|   |(PDF,etc.)|    | (Chunks) |    |(Vectors) |    |Vector DB |       |\n",
        "|   +----------+    +----------+    +----------+    +----------+       |\n",
        "|                                                                      |\n",
        "|   ONLINE (Every query):                                              |\n",
        "|   +----------+    +----------+    +----------+    +----------+       |\n",
        "|   | Question | -> |  Embed   | -> |  Search  | -> | Retrieved|       |\n",
        "|   |          |    | Question |    |  Similar |    |  Chunks  |       |\n",
        "|   +----------+    +----------+    +----------+    +----------+       |\n",
        "|                                           |                          |\n",
        "|                                           v                          |\n",
        "|                                   +--------------+                   |\n",
        "|                                   |     LLM      |                   |\n",
        "|                                   |   Combines   | -> Final Answer   |\n",
        "|                                   |   Context +  |                   |\n",
        "|                                   |   Question   |                   |\n",
        "|                                   +--------------+                   |\n",
        "|                                                                      |\n",
        "+---------------------------------------------------------------------+\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup-header",
      "metadata": {
        "id": "setup-header"
      },
      "source": [
        "---\n",
        "\n",
        "## Setup: Installing and Importing Libraries\n",
        "\n",
        "We'll use:\n",
        "- `langchain-openai`: OpenAI chat models and embeddings\n",
        "- `langchain-pinecone`: Pinecone vector store integration\n",
        "- `langchain-core`: Core LCEL components\n",
        "- `pinecone`: Vector database client\n",
        "- `datasets`: HuggingFace datasets for loading sample data\n",
        "\n",
        "> **Security**: API keys are loaded from environment variables - never hardcode them!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "from typing import List, Optional\n",
        "from time import sleep\n",
        "\n",
        "# Load environment variables\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
        "\n",
        "# History management\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "\n",
        "# Pinecone\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# Data handling\n",
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(\"All imports successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "verify-keys",
      "metadata": {
        "id": "verify-keys"
      },
      "outputs": [],
      "source": [
        "# Verify API keys are loaded\n",
        "openai_key = os.getenv('OPENAI_API_KEY')\n",
        "pinecone_key = os.getenv('PINECONE_API_KEY')\n",
        "\n",
        "if openai_key:\n",
        "    print(\"OpenAI API key loaded!\")\n",
        "else:\n",
        "    print(\"WARNING: No OpenAI API key found. Set OPENAI_API_KEY in .env file\")\n",
        "\n",
        "if pinecone_key:\n",
        "    print(\"Pinecone API key loaded!\")\n",
        "else:\n",
        "    print(\"WARNING: No Pinecone API key found. Set PINECONE_API_KEY in .env file\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "part1-header",
      "metadata": {
        "id": "part1-header"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 1: Embeddings - Turning Words into Numbers\n",
        "\n",
        "### What You'll Learn\n",
        "- What embeddings are and why they're magical\n",
        "- How similarity works in vector space\n",
        "- Using OpenAI's embedding models\n",
        "\n",
        "### Key Concept: Embeddings\n",
        "\n",
        "> **Definition**: Embeddings are numerical representations (vectors) of text that capture semantic meaning. Similar texts have similar embeddings.\n",
        ">\n",
        "> **Analogy**: Imagine a map where cities are placed by similarity:\n",
        "> - Paris and Rome are close (both European capitals)\n",
        "> - Paris and \"Eiffel Tower\" are close (related concepts)\n",
        "> - Paris and \"Banana\" are far apart (unrelated)\n",
        "\n",
        "### How Embeddings Capture Meaning\n",
        "\n",
        "```\n",
        "Text Embedding Example:\n",
        "================================================================\n",
        "\n",
        "\"King\"  -> [0.2, 0.8, 0.1, -0.3, ...]  (1536 dimensions)\n",
        "\"Queen\" -> [0.3, 0.7, 0.2, -0.2, ...]  (similar to King!)\n",
        "\"Apple\" -> [-0.5, 0.1, 0.9, 0.4, ...]  (very different)\n",
        "\n",
        "Famous equation (Word2Vec discovered this):\n",
        "King - Man + Woman ~ Queen\n",
        "```\n",
        "\n",
        "### OpenAI Embedding Models\n",
        "\n",
        "| Model | Dimensions | Best For | Cost |\n",
        "|-------|------------|----------|------|\n",
        "| `text-embedding-3-small` | 1536 | General use, cost-effective | $ |\n",
        "| `text-embedding-3-large` | 3072 | Higher accuracy | $$ |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create-embedding-model",
      "metadata": {
        "id": "create-embedding-model"
      },
      "outputs": [],
      "source": [
        "# Create an embedding model\n",
        "embedding_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
        "\n",
        "print(\"Embedding model created!\")\n",
        "print(f\"Model: text-embedding-3-small\")\n",
        "print(f\"Dimensions: 1536\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test-embeddings",
      "metadata": {
        "id": "test-embeddings"
      },
      "outputs": [],
      "source": [
        "# Let's see embeddings in action!\n",
        "\n",
        "# Embed some sample texts\n",
        "texts = [\n",
        "    \"The king sat on his throne\",\n",
        "    \"The queen wore a golden crown\",\n",
        "    \"I love eating pizza\"\n",
        "]\n",
        "\n",
        "embeddings = embedding_model.embed_documents(texts)\n",
        "\n",
        "print(\"=== Embedding Results ===\")\n",
        "for i, text in enumerate(texts):\n",
        "    print(f\"\\nText: '{text}'\")\n",
        "    print(f\"Embedding (first 5 values): {embeddings[i][:5]}\")\n",
        "    print(f\"Total dimensions: {len(embeddings[i])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "similarity-demo",
      "metadata": {
        "id": "similarity-demo"
      },
      "outputs": [],
      "source": [
        "# Let's calculate similarity between embeddings\n",
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "print(\"=== Similarity Scores ===\")\n",
        "print(f\"'King' vs 'Queen': {cosine_similarity(embeddings[0], embeddings[1]):.4f}\")\n",
        "print(f\"'King' vs 'Pizza': {cosine_similarity(embeddings[0], embeddings[2]):.4f}\")\n",
        "print(f\"'Queen' vs 'Pizza': {cosine_similarity(embeddings[1], embeddings[2]):.4f}\")\n",
        "print()\n",
        "print(\"(Higher = more similar, Range: -1 to 1)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "part2-header",
      "metadata": {
        "id": "part2-header"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 2: Vector Stores - Databases for AI\n",
        "\n",
        "### What You'll Learn\n",
        "- What vector databases are and why we need them\n",
        "- Setting up Pinecone (cloud vector database)\n",
        "- Storing and retrieving document embeddings\n",
        "\n",
        "### Why Not Use Regular Databases?\n",
        "\n",
        "```\n",
        "REGULAR DATABASE (SQL):\n",
        "---------------------------------------------------------\n",
        "SELECT * FROM documents WHERE content LIKE '%refund%'\n",
        "\n",
        "Problem: Only finds EXACT word matches!\n",
        "---------------------------------------------------------\n",
        "\n",
        "VECTOR DATABASE:\n",
        "---------------------------------------------------------\n",
        "Find documents SIMILAR to \"refund\" embedding\n",
        "\n",
        "Result: Finds ALL semantically related documents!\n",
        "- \"return policy\" -> FOUND (similar meaning)\n",
        "- \"money back guarantee\" -> FOUND (similar meaning)\n",
        "---------------------------------------------------------\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create-pinecone",
      "metadata": {
        "id": "create-pinecone"
      },
      "outputs": [],
      "source": [
        "# Initialize Pinecone client\n",
        "pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\n",
        "\n",
        "print(\"Pinecone client created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create-index",
      "metadata": {
        "id": "create-index"
      },
      "outputs": [],
      "source": [
        "# Create or connect to a Pinecone index\n",
        "index_name = \"llama2-arxiv-papers-chunked\"\n",
        "\n",
        "# Check if index already exists\n",
        "existing_indexes = [idx['name'] for idx in pc.list_indexes()]\n",
        "\n",
        "if index_name not in existing_indexes:\n",
        "    print(f\"Creating new index: {index_name}\")\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=1536,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(\n",
        "            cloud='aws',\n",
        "            region='us-east-1'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    while not pc.describe_index(index_name).status['ready']:\n",
        "        print(\"Waiting for index to be ready...\")\n",
        "        sleep(2)\n",
        "else:\n",
        "    print(f\"Index '{index_name}' already exists!\")\n",
        "\n",
        "index = pc.Index(index_name)\n",
        "print(f\"Connected to index: {index_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "check-index-stats",
      "metadata": {
        "id": "check-index-stats"
      },
      "outputs": [],
      "source": [
        "# Check index statistics\n",
        "index_stats = index.describe_index_stats()\n",
        "\n",
        "print(\"=== Index Statistics ===\")\n",
        "print(f\"Dimension: {index_stats['dimension']}\")\n",
        "print(f\"Total vectors: {index_stats['total_vector_count']}\")\n",
        "\n",
        "index_initialized = index_stats['total_vector_count'] > 0\n",
        "\n",
        "if index_initialized:\n",
        "    print(f\"\\nIndex is ready with {index_stats['total_vector_count']} vectors!\")\n",
        "else:\n",
        "    print(\"\\nIndex is empty. We'll populate it in the next section.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "part3-header",
      "metadata": {
        "id": "part3-header"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 3: Loading and Storing Documents\n",
        "\n",
        "### Why Chunk Documents?\n",
        "\n",
        "```\n",
        "Problem: A 100-page PDF is too large to embed as one vector\n",
        "\n",
        "Solution: Split into smaller \"chunks\" (e.g., 500-1000 characters each)\n",
        "\n",
        "           Original Document              Chunked Document\n",
        "         +------------------+           +--------+\n",
        "         |   100 pages      |   -->     | Chunk 1| -> Vector 1\n",
        "         |   of text        |           +--------+\n",
        "         +------------------+           | Chunk 2| -> Vector 2\n",
        "                                        +--------+\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-dataset",
      "metadata": {
        "id": "load-dataset"
      },
      "outputs": [],
      "source": [
        "# Load a sample dataset (already chunked!)\n",
        "print(\"Loading dataset...\")\n",
        "llama2_dataset = load_dataset(\n",
        "    \"jamescalam/llama-2-arxiv-papers-chunked\",\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "print(f\"\\n=== Dataset Info ===\")\n",
        "print(f\"Number of chunks: {len(llama2_dataset)}\")\n",
        "print(f\"Features: {list(llama2_dataset.features.keys())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "explore-dataset",
      "metadata": {
        "id": "explore-dataset"
      },
      "outputs": [],
      "source": [
        "# Let's look at one example\n",
        "example = llama2_dataset[0]\n",
        "\n",
        "print(\"=== Example Chunk ===\")\n",
        "print(f\"Title: {example['title']}\")\n",
        "print(f\"DOI: {example['doi']}\")\n",
        "print(f\"Chunk ID: {example['chunk-id']}\")\n",
        "print(f\"\\nChunk content (first 300 chars):\")\n",
        "print(example['chunk'][:300] + \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "upload-vectors",
      "metadata": {
        "id": "upload-vectors"
      },
      "outputs": [],
      "source": [
        "# Upload vectors to Pinecone (only if not already done)\n",
        "\n",
        "if not index_initialized:\n",
        "    print(\"Uploading vectors to Pinecone...\")\n",
        "    print(\"This may take a few minutes...\\n\")\n",
        "\n",
        "    df = llama2_dataset.to_pandas()\n",
        "    batch_size = 128\n",
        "\n",
        "    for i in tqdm(range(0, len(df), batch_size)):\n",
        "        batch = df.iloc[i:i + batch_size]\n",
        "        ids = [f\"{row['doi']}-{row['chunk-id']}\" for _, row in batch.iterrows()]\n",
        "        texts = [row['chunk'] for _, row in batch.iterrows()]\n",
        "        embeddings = embedding_model.embed_documents(texts)\n",
        "        metadata = [\n",
        "            {\n",
        "                'text': row['chunk'],\n",
        "                'source': row['source'],\n",
        "                'title': row['title'],\n",
        "            } for _, row in batch.iterrows()\n",
        "        ]\n",
        "        index.upsert(vectors=list(zip(ids, embeddings, metadata)))\n",
        "\n",
        "    print(\"\\nUpload complete!\")\n",
        "else:\n",
        "    print(\"Index already populated. Skipping upload.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "part4-header",
      "metadata": {
        "id": "part4-header"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: Building a Basic RAG Chain with LCEL\n",
        "\n",
        "### What You'll Learn\n",
        "- Creating a retriever from a vector store\n",
        "- Building a RAG chain using pure LCEL\n",
        "- Understanding the retrieval + generation pattern"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create-vector-store",
      "metadata": {
        "id": "create-vector-store"
      },
      "outputs": [],
      "source": [
        "# Create a LangChain vector store wrapper\n",
        "vector_store = PineconeVectorStore(\n",
        "    index=index,\n",
        "    embedding=embedding_model,\n",
        "    text_key='text'\n",
        ")\n",
        "\n",
        "print(\"Vector store created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create-retriever",
      "metadata": {
        "id": "create-retriever"
      },
      "outputs": [],
      "source": [
        "# Create a retriever\n",
        "retriever = vector_store.as_retriever(\n",
        "    search_kwargs={\"k\": 3}\n",
        ")\n",
        "\n",
        "print(\"Retriever created!\")\n",
        "print(\"Configuration: Returns top 3 most similar documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test-retriever",
      "metadata": {
        "id": "test-retriever"
      },
      "outputs": [],
      "source": [
        "# Test the retriever\n",
        "query = \"What is LLaMA-2?\"\n",
        "\n",
        "print(f\"=== Testing Retriever ===\")\n",
        "print(f\"Query: {query}\\n\")\n",
        "\n",
        "docs = retriever.invoke(query)\n",
        "\n",
        "for i, doc in enumerate(docs, 1):\n",
        "    print(f\"--- Document {i} ---\")\n",
        "    print(f\"Content (first 200 chars): {doc.page_content[:200]}...\")\n",
        "    print(f\"Source: {doc.metadata.get('source', 'N/A')}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create-llm",
      "metadata": {
        "id": "create-llm"
      },
      "outputs": [],
      "source": [
        "# Create the chat model\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-5-mini\",\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "print(\"Chat model created!\")\n",
        "print(f\"Model: gpt-5-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "part5-header",
      "metadata": {
        "id": "part5-header"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 5: History-Aware RAG - Handling Follow-Up Questions\n",
        "\n",
        "### The Problem: Follow-Up Questions Fail\n",
        "\n",
        "```\n",
        "BASIC RAG PROBLEM:\n",
        "================================================================\n",
        "\n",
        "Q1: \"What is LLaMA-2?\"\n",
        "    -> Embeds: \"What is LLaMA-2?\" [OK]\n",
        "    -> Retrieves: LLaMA-2 docs [OK]\n",
        "\n",
        "Q2: \"What are its main features?\"\n",
        "    -> Embeds: \"What are its main features?\" [X]\n",
        "    -> \"its\" = ??? (no context!)\n",
        "    -> Retrieves: Random \"features\" docs [X]\n",
        "\n",
        "================================================================\n",
        "```\n",
        "\n",
        "### The Solution: Query Rewriting + RunnableWithMessageHistory\n",
        "\n",
        "> **Key Idea**: LangChain provides `RunnableWithMessageHistory` to automatically\n",
        "> manage conversation history. Combined with a query rewriter, we get pure LCEL history-aware RAG.\n",
        "\n",
        "```\n",
        "HISTORY-AWARE RAG (Pure LCEL Pattern):\n",
        "================================================================\n",
        "\n",
        "Q2: \"What are its main features?\"\n",
        "\n",
        "    Step 1: RunnableWithMessageHistory provides chat_history automatically\n",
        "    +----------------------------------------------------------+\n",
        "    | Chat History (managed by RunnableWithMessageHistory):    |\n",
        "    |   User: \"What is LLaMA-2?\"                               |\n",
        "    |   AI: \"LLaMA-2 is Meta's open-source LLM...\"             |\n",
        "    +----------------------------------------------------------+\n",
        "\n",
        "    Step 2: Query Rewriter (LCEL chain)\n",
        "    +----------------------------------------------------------+\n",
        "    | Input: \"What are its main features?\"                     |\n",
        "    | Output: \"What are the main features of LLaMA-2?\"         |\n",
        "    +----------------------------------------------------------+\n",
        "\n",
        "    Step 3: Search with rewritten question\n",
        "    -> Embeds: \"What are the main features of LLaMA-2?\" [OK]\n",
        "    -> Retrieves: LLaMA-2 feature docs [OK]\n",
        "\n",
        "================================================================\n",
        "```\n",
        "\n",
        "### Architecture: Pure LCEL with RunnableWithMessageHistory\n",
        "\n",
        "```\n",
        "+---------------------------------------------------------------------+\n",
        "|              PURE LCEL HISTORY-AWARE RAG                            |\n",
        "+---------------------------------------------------------------------+\n",
        "|                                                                     |\n",
        "|   rag_chain = (                                                     |\n",
        "|       RunnablePassthrough.assign(                                   |\n",
        "|           standalone=rewrite_prompt | llm | parser   # Query rewrite|\n",
        "|       )                                                             |\n",
        "|       | RunnablePassthrough.assign(                                 |\n",
        "|           context=lambda x: retriever.invoke(x[\"standalone\"])       |\n",
        "|       )                                                             |\n",
        "|       | qa_prompt | llm | parser                     # Generate    |\n",
        "|   )                                                                 |\n",
        "|                                                                     |\n",
        "|   rag_with_history = RunnableWithMessageHistory(                    |\n",
        "|       rag_chain,                                                    |\n",
        "|       get_session_history,          # Session-based history store   |\n",
        "|       input_messages_key=\"input\",                                   |\n",
        "|       history_messages_key=\"chat_history\"                           |\n",
        "|   )                                                                 |\n",
        "|                                                                     |\n",
        "|   Usage:                                                            |\n",
        "|   response = rag_with_history.invoke(                               |\n",
        "|       {\"input\": \"What are its features?\"},                          |\n",
        "|       config={\"configurable\": {\"session_id\": \"user1\"}}              |\n",
        "|   )                                                                 |\n",
        "|                                                                     |\n",
        "+---------------------------------------------------------------------+\n",
        "```\n",
        "\n",
        "### Why Pure LCEL?\n",
        "\n",
        "| Approach | Pros | Cons |\n",
        "|----------|------|------|\n",
        "| **Class Wrapper** | Explicit, easy to debug | More verbose, less composable |\n",
        "| **Pure LCEL** | Composable, idiomatic, concise | Requires understanding LCEL |\n",
        "\n",
        "**Pure LCEL is the recommended approach!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "query-rewriter",
      "metadata": {
        "id": "query-rewriter"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STEP 1: Create the Query Rewriter (LCEL Chain)\n",
        "# =============================================================================\n",
        "# This chain rewrites follow-up questions to be standalone\n",
        "\n",
        "rewrite_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
        "which might reference context in the chat history, formulate a standalone question \\\n",
        "which can be understood without the chat history. Do NOT answer the question, \\\n",
        "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
        "\n",
        "rewrite_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", rewrite_system_prompt),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "# LCEL chain: prompt -> llm -> parser\n",
        "query_rewriter = rewrite_prompt | llm | StrOutputParser()\n",
        "\n",
        "print(\"Query rewriter chain created (LCEL)!\")\n",
        "print(\"\\nThis will rewrite questions like:\")\n",
        "print('  \"What are its features?\" -> \"What are the features of LLaMA-2?\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test-rewriter",
      "metadata": {
        "id": "test-rewriter"
      },
      "outputs": [],
      "source": [
        "# Test the query rewriter\n",
        "test_history = [\n",
        "    HumanMessage(content=\"What is LLaMA-2?\"),\n",
        "    AIMessage(content=\"LLaMA-2 is Meta's open-source large language model.\")\n",
        "]\n",
        "\n",
        "rewritten = query_rewriter.invoke({\n",
        "    \"chat_history\": test_history,\n",
        "    \"input\": \"What are its main features?\"\n",
        "})\n",
        "\n",
        "print(\"=== Query Rewriter Test ===\")\n",
        "print(f\"Original: 'What are its main features?'\")\n",
        "print(f\"Rewritten: '{rewritten}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qa-chain",
      "metadata": {
        "id": "qa-chain"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STEP 2: Create the QA Chain (LCEL)\n",
        "# =============================================================================\n",
        "# This chain generates answers from retrieved context\n",
        "\n",
        "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
        "Use the following pieces of retrieved context to answer the question. \\\n",
        "If you don't know the answer, just say that you don't know. \\\n",
        "Use three sentences maximum and keep the answer concise.\n",
        "\n",
        "Context:\n",
        "{context}\"\"\"\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", qa_system_prompt),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "# Helper function to format documents\n",
        "def format_docs(docs):\n",
        "    \"\"\"Convert Document objects to a formatted string.\"\"\"\n",
        "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "print(\"QA prompt created!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "history-aware-rag-class",
      "metadata": {
        "id": "history-aware-rag-class"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STEP 3: Build Pure LCEL History-Aware RAG Chain\n",
        "# =============================================================================\n",
        "# This is the recommended pattern using RunnableWithMessageHistory\n",
        "\n",
        "# Pure LCEL chain: rewrite -> retrieve -> generate\n",
        "rag_chain = (\n",
        "    # Step 1: Rewrite the question to be standalone\n",
        "    RunnablePassthrough.assign(\n",
        "        standalone=rewrite_prompt | llm | StrOutputParser()\n",
        "    )\n",
        "    # Step 2: Retrieve documents using the rewritten question\n",
        "    | RunnablePassthrough.assign(\n",
        "        context=lambda x: format_docs(retriever.invoke(x[\"standalone\"]))\n",
        "    )\n",
        "    # Step 3: Generate answer\n",
        "    | qa_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"Pure LCEL RAG chain created!\")\n",
        "print(\"\\nChain flow:\")\n",
        "print(\"  1. Rewrite question (if history exists)\")\n",
        "print(\"  2. Retrieve documents with rewritten question\")\n",
        "print(\"  3. Generate answer from context\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test-history-aware-rag",
      "metadata": {
        "id": "test-history-aware-rag"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STEP 4: Wrap with RunnableWithMessageHistory\n",
        "# =============================================================================\n",
        "# This automatically manages chat history per session\n",
        "\n",
        "# Session history store (in-memory, could be Redis/DB in production)\n",
        "session_store = {}\n",
        "\n",
        "def get_session_history(session_id: str) -> ChatMessageHistory:\n",
        "    \"\"\"Get or create chat history for a session.\"\"\"\n",
        "    if session_id not in session_store:\n",
        "        session_store[session_id] = ChatMessageHistory()\n",
        "    return session_store[session_id]\n",
        "\n",
        "# Wrap the RAG chain with history management\n",
        "rag_with_history = RunnableWithMessageHistory(\n",
        "    rag_chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\"\n",
        ")\n",
        "\n",
        "print(\"History-aware RAG created (pure LCEL)!\")\n",
        "print(\"\\nUsage:\")\n",
        "print('  response = rag_with_history.invoke(')\n",
        "print('      {\"input\": \"Your question\"},')\n",
        "print('      config={\"configurable\": {\"session_id\": \"user1\"}}')\n",
        "print('  )')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test-followup",
      "metadata": {
        "id": "test-followup"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TEST: History-Aware RAG in Action!\n",
        "# =============================================================================\n",
        "\n",
        "# Configuration for this conversation session\n",
        "config = {\"configurable\": {\"session_id\": \"demo-session\"}}\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"TESTING PURE LCEL HISTORY-AWARE RAG\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Question 1 - First question\n",
        "question1 = \"What is LLaMA-2?\"\n",
        "print(f\"\\nUser: {question1}\")\n",
        "\n",
        "answer1 = rag_with_history.invoke(\n",
        "    {\"input\": question1},\n",
        "    config=config\n",
        ")\n",
        "\n",
        "print(f\"\\nAI: {answer1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test-more-followups",
      "metadata": {
        "id": "test-more-followups"
      },
      "outputs": [],
      "source": [
        "# Question 2 - Follow-up using \"its\" (should work now!)\n",
        "question2 = \"What are its main features?\"\n",
        "print(f\"\\nUser: {question2}\")\n",
        "print(\"       ^ Note: 'its' refers to LLaMA-2 from previous question\")\n",
        "\n",
        "answer2 = rag_with_history.invoke(\n",
        "    {\"input\": question2},\n",
        "    config=config  # Same session_id - history is preserved!\n",
        ")\n",
        "\n",
        "print(f\"\\nAI: {answer2}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"SUCCESS! The follow-up question was correctly understood!\")\n",
        "print(\"RunnableWithMessageHistory automatically managed the chat history.\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "view-chat-history",
      "metadata": {
        "id": "view-chat-history"
      },
      "outputs": [],
      "source": [
        "# Question 3 - Another follow-up\n",
        "question3 = \"How was it trained?\"\n",
        "print(f\"\\nUser: {question3}\")\n",
        "\n",
        "answer3 = rag_with_history.invoke(\n",
        "    {\"input\": question3},\n",
        "    config=config\n",
        ")\n",
        "\n",
        "print(f\"\\nAI: {answer3}\")\n",
        "\n",
        "# View the complete chat history\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"COMPLETE CONVERSATION HISTORY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "history = session_store[\"demo-session\"].messages\n",
        "for i, msg in enumerate(history):\n",
        "    role = \"User\" if isinstance(msg, HumanMessage) else \"AI\"\n",
        "    content = msg.content[:150] + '...' if len(msg.content) > 150 else msg.content\n",
        "    print(f\"\\n{role}: {content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "part6-header",
      "metadata": {
        "id": "part6-header"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 6: Advanced RAG with LangGraph\n",
        "\n",
        "### What You'll Learn\n",
        "- Why LangGraph gives more control over RAG\n",
        "- Building a stateful RAG pipeline with explicit nodes\n",
        "- Using checkpointing for persistent conversations\n",
        "\n",
        "### Why LangGraph for RAG?\n",
        "\n",
        "The LCEL class approach above is clean, but LangGraph offers:\n",
        "- **Visual workflow**: Clear node-based architecture\n",
        "- **Built-in persistence**: Automatic state checkpointing\n",
        "- **Conditional routing**: Add quality checks, retry logic\n",
        "- **Multi-step workflows**: Complex RAG patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "langgraph-imports",
      "metadata": {
        "id": "langgraph-imports"
      },
      "outputs": [],
      "source": [
        "# Import LangGraph components\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from typing import TypedDict, Annotated, Optional\n",
        "\n",
        "print(\"LangGraph components imported!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "langgraph-state",
      "metadata": {
        "id": "langgraph-state"
      },
      "outputs": [],
      "source": [
        "# Define the state schema for our RAG workflow\n",
        "\n",
        "class RAGState(TypedDict):\n",
        "    \"\"\"State that flows through our RAG graph.\"\"\"\n",
        "    question: str\n",
        "    standalone_question: Optional[str]\n",
        "    documents: Optional[str]\n",
        "    messages: Annotated[list, add_messages]\n",
        "    answer: Optional[str]\n",
        "\n",
        "print(\"RAG state schema defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "langgraph-nodes",
      "metadata": {
        "id": "langgraph-nodes"
      },
      "outputs": [],
      "source": [
        "# Define the nodes (functions that process state)\n",
        "\n",
        "def rewrite_node(state: RAGState) -> dict:\n",
        "    \"\"\"Rewrite the query to be standalone based on chat history.\"\"\"\n",
        "    print(\"  [Rewrite Node] Processing...\")\n",
        "\n",
        "    question = state[\"question\"]\n",
        "    messages = state.get(\"messages\", [])\n",
        "\n",
        "    if not messages:\n",
        "        print(\"  [Rewrite Node] No history, using original question\")\n",
        "        return {\"standalone_question\": question}\n",
        "\n",
        "    rewritten = query_rewriter.invoke({\n",
        "        \"chat_history\": messages,\n",
        "        \"input\": question\n",
        "    })\n",
        "\n",
        "    print(f\"  [Rewrite Node] '{question}' -> '{rewritten}'\")\n",
        "    return {\"standalone_question\": rewritten}\n",
        "\n",
        "\n",
        "def retrieve_node(state: RAGState) -> dict:\n",
        "    \"\"\"Retrieve relevant documents from the vector store.\"\"\"\n",
        "    print(\"  [Retrieve Node] Searching...\")\n",
        "\n",
        "    query = state.get(\"standalone_question\") or state[\"question\"]\n",
        "    docs = retriever.invoke(query)\n",
        "    context = format_docs(docs)\n",
        "\n",
        "    print(f\"  [Retrieve Node] Found {len(docs)} documents\")\n",
        "    return {\"documents\": context}\n",
        "\n",
        "\n",
        "def generate_node(state: RAGState) -> dict:\n",
        "    \"\"\"Generate an answer using the LLM and retrieved context.\"\"\"\n",
        "    print(\"  [Generate Node] Generating answer...\")\n",
        "\n",
        "    question = state[\"question\"]\n",
        "    context = state.get(\"documents\", \"No context available.\")\n",
        "    messages = state.get(\"messages\", [])\n",
        "\n",
        "    chain = qa_prompt | llm | StrOutputParser()\n",
        "    answer = chain.invoke({\n",
        "        \"context\": context,\n",
        "        \"chat_history\": messages,\n",
        "        \"input\": question\n",
        "    })\n",
        "\n",
        "    print(\"  [Generate Node] Done!\")\n",
        "    return {\n",
        "        \"answer\": answer,\n",
        "        \"messages\": [\n",
        "            HumanMessage(content=question),\n",
        "            AIMessage(content=answer)\n",
        "        ]\n",
        "    }\n",
        "\n",
        "print(\"RAG nodes defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "langgraph-build",
      "metadata": {
        "id": "langgraph-build"
      },
      "outputs": [],
      "source": [
        "# Build the RAG graph\n",
        "\n",
        "rag_workflow = StateGraph(RAGState)\n",
        "\n",
        "# Add nodes\n",
        "rag_workflow.add_node(\"rewrite\", rewrite_node)\n",
        "rag_workflow.add_node(\"retrieve\", retrieve_node)\n",
        "rag_workflow.add_node(\"generate\", generate_node)\n",
        "\n",
        "# Add edges\n",
        "rag_workflow.add_edge(START, \"rewrite\")\n",
        "rag_workflow.add_edge(\"rewrite\", \"retrieve\")\n",
        "rag_workflow.add_edge(\"retrieve\", \"generate\")\n",
        "rag_workflow.add_edge(\"generate\", END)\n",
        "\n",
        "# Compile with memory\n",
        "memory_saver = MemorySaver()\n",
        "rag_graph = rag_workflow.compile(checkpointer=memory_saver)\n",
        "\n",
        "print(\"LangGraph RAG compiled!\")\n",
        "print(\"\\nGraph: START -> rewrite -> retrieve -> generate -> END\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test-langgraph-rag",
      "metadata": {
        "id": "test-langgraph-rag"
      },
      "outputs": [],
      "source": [
        "# Test the LangGraph RAG\n",
        "config = {\"configurable\": {\"thread_id\": \"user-1\"}}\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"TESTING LANGGRAPH HISTORY-AWARE RAG\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "question = \"What is the training data size for LLaMA-2?\"\n",
        "print(f\"\\nUser: {question}\")\n",
        "\n",
        "result = rag_graph.invoke(\n",
        "    {\n",
        "        \"question\": question,\n",
        "        \"standalone_question\": None,\n",
        "        \"documents\": None,\n",
        "        \"messages\": [],\n",
        "        \"answer\": None\n",
        "    },\n",
        "    config=config\n",
        ")\n",
        "\n",
        "print(f\"\\nAI: {result['answer']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test-langgraph-followup",
      "metadata": {
        "id": "test-langgraph-followup"
      },
      "outputs": [],
      "source": [
        "# Follow-up question (LangGraph remembers via checkpointer)\n",
        "question2 = \"How does it compare to the original LLaMA?\"\n",
        "print(f\"\\nUser: {question2}\")\n",
        "print(\"       ^ Note: 'it' refers to LLaMA-2\")\n",
        "\n",
        "result2 = rag_graph.invoke(\n",
        "    {\n",
        "        \"question\": question2,\n",
        "        \"standalone_question\": None,\n",
        "        \"documents\": None,\n",
        "        \"messages\": [],\n",
        "        \"answer\": None\n",
        "    },\n",
        "    config=config  # Same thread_id\n",
        ")\n",
        "\n",
        "print(f\"\\nAI: {result2['answer']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "---\n",
        "\n",
        "## Summary: What You've Learned\n",
        "\n",
        "Congratulations! You've learned how to build a RAG-powered chatbot that handles follow-up questions correctly using **pure LCEL**.\n",
        "\n",
        "| Concept | What It Does | LangChain Approach |\n",
        "|---------|--------------|-------------------|\n",
        "| **Embeddings** | Turn text into vectors | `OpenAIEmbeddings` |\n",
        "| **Vector Stores** | Store and search vectors | `PineconeVectorStore` |\n",
        "| **Retrievers** | Find relevant documents | `vector_store.as_retriever()` |\n",
        "| **Query Rewriting** | Handle follow-up questions | LCEL: `prompt \\| llm \\| parser` |\n",
        "| **History Management** | Track conversation | `RunnableWithMessageHistory` |\n",
        "| **RAG Pipeline** | Combine retrieval + generation | Pure LCEL chain composition |\n",
        "| **Advanced RAG** | Stateful workflows | LangGraph `StateGraph` |\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Pure LCEL is the way**: Use `RunnablePassthrough.assign()` to build composable chains\n",
        "2. **RunnableWithMessageHistory**: Automatic session-based history management\n",
        "3. **Query rewriting is essential**: Follow-up questions need context from history\n",
        "4. **LangGraph adds control**: Build complex workflows with explicit state management\n",
        "\n",
        "### The Pure LCEL Pattern\n",
        "\n",
        "```python\n",
        "# Build the chain\n",
        "rag_chain = (\n",
        "    RunnablePassthrough.assign(standalone=rewrite_chain)\n",
        "    | RunnablePassthrough.assign(context=retrieve_fn)\n",
        "    | qa_prompt | llm | parser\n",
        ")\n",
        "\n",
        "# Wrap with history\n",
        "rag_with_history = RunnableWithMessageHistory(\n",
        "    rag_chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\"\n",
        ")\n",
        "\n",
        "# Use it\n",
        "response = rag_with_history.invoke(\n",
        "    {\"input\": \"What are its features?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"user1\"}}\n",
        ")\n",
        "```\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [LCEL Documentation](https://python.langchain.com/docs/concepts/lcel/)\n",
        "- [RunnableWithMessageHistory](https://python.langchain.com/docs/how_to/message_history/)\n",
        "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
        "- [Pinecone Documentation](https://docs.pinecone.io/)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}